{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff354f2d-7f9a-46f5-b549-d3775d819472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook 05: Model Evaluation\n",
    "Comprehensive model evaluation, business impact analysis, and deployment recommendations\n",
    "Serverless-compatible version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e67502-a1c4-41eb-8fb9-cd12ae8239ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3964104-66fe-4331-b75b-77f9f152b9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hotel_Churn_Model_Evaluation\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de812639-63f1-429d-b2c2-fe1acf78de2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe268d9-0f23-4cdb-a7fc-62c65409ae47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"Loading test data...\")\n",
    "\n",
    "# Try to load test set from previous notebook\n",
    "try:\n",
    "    test_df = spark.table(\"hotel_catalog.gold.model_test_set\")\n",
    "    print(\"Loaded test set from gold.model_test_set\")\n",
    "    print(f\"Test set size: {test_df.count():,} records\")\n",
    "except:\n",
    "    # Try to load from features table and create test split\n",
    "    try:\n",
    "        features_df = spark.table(\"hotel_catalog.gold.hotel_features_final\")\n",
    "        # Create 70/15/15 split if not done before\n",
    "        train_df, val_df, test_df = features_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "        print(\"Created new train/val/test split\")\n",
    "        print(f\"Test set size: {test_df.count():,} records\")\n",
    "    except:\n",
    "        # Fallback: use simple features\n",
    "        silver_df = spark.table(\"hotel_catalog.silver.cleaned_hotel_bookings\")\n",
    "        \n",
    "        # Create simple features\n",
    "        features_df = silver_df.select(\n",
    "            \"hotel\", \"churn\", \"lead_time\", \"arrival_date_year\",\n",
    "            \"arrival_date_month\", \"stays_in_weekend_nights\", \"stays_in_week_nights\",\n",
    "            \"adults\", col(\"children\").cast(\"double\"), \"babies\",\n",
    "            \"previous_cancellations\", \"adr\", \"deposit_type\", \"customer_type\"\n",
    "        ).fillna(0)\n",
    "        \n",
    "        features_df = features_df.withColumn(\n",
    "            \"total_nights\", col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")\n",
    "        ).withColumn(\n",
    "            \"total_guests\", col(\"adults\") + col(\"children\") + col(\"babies\")\n",
    "        ).withColumn(\n",
    "            \"is_weekend_stay\", when(col(\"stays_in_weekend_nights\") > 0, 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"hotel_code\", when(col(\"hotel\") == \"Resort Hotel\", 0).otherwise(1)\n",
    "        ).withColumn(\n",
    "            \"deposit_code\", \n",
    "            when(col(\"deposit_type\") == \"No Deposit\", 0)\n",
    "            .when(col(\"deposit_type\") == \"Non Refund\", 1)\n",
    "            .otherwise(2)\n",
    "        )\n",
    "        \n",
    "        # Create feature vector\n",
    "        from pyspark.ml.feature import VectorAssembler\n",
    "        feature_cols = [\"lead_time\", \"total_nights\", \"total_guests\", \n",
    "                       \"previous_cancellations\", \"adr\", \"is_weekend_stay\", \n",
    "                       \"hotel_code\", \"deposit_code\"]\n",
    "        \n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=feature_cols,\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        features_df = assembler.transform(features_df)\n",
    "        train_df, val_df, test_df = features_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "        print(\"Created features and test split from silver data\")\n",
    "        print(f\"Test set size: {test_df.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0365d2ab-8b89-4fba-a61e-5bf9b4134b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecff71c-9268-45e5-963b-ea4f3c4844a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_name = \"hotel_churn_predictor\"\n",
    "best_model = None\n",
    "model_source = None\n",
    "\n",
    "# Try multiple methods to load the model\n",
    "try:\n",
    "    # Method 1: Load from Model Registry (Production/Staging)\n",
    "    model_versions = mlflow_client.get_latest_versions(model_name, stages=[\"Production\", \"Staging\"])\n",
    "    if model_versions:\n",
    "        model_version = model_versions[0]\n",
    "        model_uri = f\"models:/{model_name}/{model_version.version}\"\n",
    "        best_model = mlflow.spark.load_model(model_uri)\n",
    "        model_source = f\"Model Registry (Version {model_version.version}, Stage: {model_version.current_stage})\"\n",
    "        print(f\"Loaded model from Model Registry: {model_uri}\")\n",
    "    else:\n",
    "        # Method 2: Search for best run in MLflow\n",
    "        experiment = mlflow.get_experiment_by_name(\"/Shared/hotel_churn_prediction\")\n",
    "        if experiment is None:\n",
    "            experiment = mlflow.get_experiment_by_name(\"hotel_churn_prediction\")\n",
    "        \n",
    "        if experiment:\n",
    "            runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "            if len(runs) > 0:\n",
    "                # Find run with highest validation AUC\n",
    "                best_run = runs.sort_values(\"metrics.val_auc\", ascending=False).iloc[0]\n",
    "                model_uri = f\"runs:/{best_run.run_id}/model\"\n",
    "                best_model = mlflow.spark.load_model(model_uri)\n",
    "                model_source = f\"MLflow Run: {best_run.run_id} (AUC: {best_run['metrics.val_auc']:.4f})\"\n",
    "                print(f\"Loaded model from MLflow run: {best_run.run_id}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load model from MLflow: {e}\")\n",
    "\n",
    "# Fallback: Use a simple model if MLflow fails\n",
    "if best_model is None:\n",
    "    print(\"Creating a simple model as fallback...\")\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "    # Train a simple model on training data\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"churn\",\n",
    "        maxIter=10\n",
    "    )\n",
    "    \n",
    "    # Use a sample for training\n",
    "    train_sample = train_df.limit(10000)\n",
    "    best_model = lr.fit(train_sample)\n",
    "    model_source = \"Fallback: Simple Logistic Regression\"\n",
    "    print(\"Created fallback model\")\n",
    "\n",
    "print(f\"Model source: {model_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef18871-f49e-4855-aeb5-31ecdc389f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1fbd83-b0e3-4a18-89d1-a956cd800222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Minimal fix: UDF to extract class 1 probability from probability vector\n",
    "get_churn_prob_udf = udf(lambda v: float(v[1]) if v is not None and len(v) > 1 else None, DoubleType())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "predictions = best_model.transform(test_df)\n",
    "\n",
    "# Add probability columns using UDF\n",
    "predictions = predictions.withColumn(\n",
    "    \"churn_probability\",\n",
    "    get_churn_prob_udf(col(\"probability\"))  # Probability of class 1 (canceled)\n",
    ").withColumn(\n",
    "    \"predicted_label\",\n",
    "    when(col(\"churn_probability\") > 0.5, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"prediction_correct\",\n",
    "    when(col(\"predicted_label\") == col(\"churn\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Predictions generated:\")\n",
    "predictions.select(\n",
    "    \"hotel\", \"churn\", \"predicted_label\", \n",
    "    \"churn_probability\", \"prediction_correct\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996d79d9-f24c-4bf1-9f02-7e7f5877a39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calculate comprehensive metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df926cca-6367-48c3-b2ef-414a23212a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Calculate metrics\n",
    "    test_auc = evaluator_auc.evaluate(predictions)\n",
    "    test_auprc = evaluator_pr.evaluate(predictions)\n",
    "    metrics_ok = True\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating metrics: {e}\")\n",
    "    print(\"This usually means the model was trained on only one class (all churn=0 or all churn=1). BinaryClassificationEvaluator requires both classes in the training data.\")\n",
    "    test_auc = None\n",
    "    test_auprc = None\n",
    "    metrics_ok = False\n",
    "\n",
    "# Calculate confusion matrix values\n",
    "TP = predictions.filter((col(\"churn\") == 1) & (col(\"predicted_label\") == 1)).count()\n",
    "TN = predictions.filter((col(\"churn\") == 0) & (col(\"predicted_label\") == 0)).count()\n",
    "FP = predictions.filter((col(\"churn\") == 0) & (col(\"predicted_label\") == 1)).count()\n",
    "FN = predictions.filter((col(\"churn\") == 1) & (col(\"predicted_label\") == 0)).count()\n",
    "\n",
    "# Calculate derived metrics\n",
    "total = TP + TN + FP + FN\n",
    "accuracy = (TP + TN) / total if total > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Calculate prevalence\n",
    "actual_positives = predictions.filter(col(\"churn\") == 1).count()\n",
    "prevalence = actual_positives / total if total > 0 else 0\n",
    "\n",
    "print(f\"Test Set Size: {total:,}\")\n",
    "print(f\"Actual Cancellations: {actual_positives:,} ({prevalence:.1%})\")\n",
    "if metrics_ok:\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"AUC-ROC:            {test_auc:.4f}\")\n",
    "    print(f\"AUC-PR:             {test_auprc:.4f}\")\n",
    "else:\n",
    "    print(\"\\nPerformance Metrics: Cannot compute AUC/PR due to single-class model.\")\n",
    "print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "print(f\"Precision:          {precision:.4f}\")\n",
    "print(f\"Recall:             {recall:.4f}\")\n",
    "print(f\"F1-Score:           {f1_score:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                   Predicted\")\n",
    "print(f\"                   No     Yes\")\n",
    "print(f\"Actual No   [{TN:6d}  {FP:6d}] â†’ {TN+FP:6d}\")\n",
    "print(f\"Actual Yes  [{FN:6d}  {TP:6d}] â†’ {FN+TP:6d}\")\n",
    "print(f\"               {TN+FN:6d}  {FP+TP:6d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c6cf61-45b2-4258-b3bc-0387b8d496ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b2af79-15d1-46aa-8a9a-8dcc38e3eaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert probabilities to pandas for threshold analysis\n",
    "prob_df = predictions.select(\"churn\", \"churn_probability\").toPandas()\n",
    "\n",
    "# Calculate metrics at different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    pred_labels = (prob_df[\"churn_probability\"] >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    TP_t = ((prob_df[\"churn\"] == 1) & (pred_labels == 1)).sum()\n",
    "    TN_t = ((prob_df[\"churn\"] == 0) & (pred_labels == 0)).sum()\n",
    "    FP_t = ((prob_df[\"churn\"] == 0) & (pred_labels == 1)).sum()\n",
    "    FN_t = ((prob_df[\"churn\"] == 1) & (pred_labels == 0)).sum()\n",
    "    \n",
    "    # Calculate metrics (cast to Python float)\n",
    "    precision_t = float(TP_t / (TP_t + FP_t)) if (TP_t + FP_t) > 0 else 0.0\n",
    "    recall_t = float(TP_t / (TP_t + FN_t)) if (TP_t + FN_t) > 0 else 0.0\n",
    "    f1_t = float(2 * precision_t * recall_t / (precision_t + recall_t)) if (precision_t + recall_t) > 0 else 0.0\n",
    "    \n",
    "    results.append({\n",
    "        \"threshold\": float(threshold),\n",
    "        \"precision\": precision_t,\n",
    "        \"recall\": recall_t,\n",
    "        \"f1_score\": f1_t,\n",
    "        \"TP\": int(TP_t),\n",
    "        \"FP\": int(FP_t),\n",
    "        \"FN\": int(FN_t)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_df = spark.createDataFrame(results)\n",
    "\n",
    "print(\"Performance at Different Thresholds:\")\n",
    "threshold_df.select(\"threshold\", \"precision\", \"recall\", \"f1_score\", \"TP\", \"FP\", \"FN\").show()\n",
    "\n",
    "# Find optimal threshold (maximizing F1)\n",
    "optimal_row = threshold_df.orderBy(col(\"f1_score\").desc()).first()\n",
    "optimal_threshold = optimal_row[\"threshold\"] if optimal_row else 0.5\n",
    "\n",
    "print(f\"\\n Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"   F1-Score at optimal: {optimal_row['f1_score']:.4f}\")\n",
    "print(f\"   Precision at optimal: {optimal_row['precision']:.4f}\")\n",
    "print(f\"   Recall at optimal: {optimal_row['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8888841f-6c6a-47fa-809c-458d2bfcb36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Business impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203e6d26-77f6-4aa1-8a50-501cfca57a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Business assumptions (simplified)\n",
    "AVERAGE_BOOKING_VALUE = 150  # Average revenue per booking\n",
    "RETENTION_COST = 25  # Cost to retain a customer (discount, upgrade, etc.)\n",
    "FALSE_POSITIVE_COST = RETENTION_COST  # Cost of unnecessary retention efforts\n",
    "FALSE_NEGATIVE_COST = AVERAGE_BOOKING_VALUE * 0.5  # Lost revenue opportunity\n",
    "\n",
    "print(\"Business Assumptions:\")\n",
    "print(f\"  â€¢ Average booking value: ${AVERAGE_BOOKING_VALUE}\")\n",
    "print(f\"  â€¢ Cost of retention action: ${RETENTION_COST}\")\n",
    "print(f\"  â€¢ Cost of false positive (unnecessary retention): ${FALSE_POSITIVE_COST}\")\n",
    "print(f\"  â€¢ Cost of false negative (missed cancellation): ${FALSE_NEGATIVE_COST}\")\n",
    "\n",
    "# Calculate costs\n",
    "total_fp_cost = FP * FALSE_POSITIVE_COST\n",
    "total_fn_cost = FN * FALSE_NEGATIVE_COST\n",
    "total_misclassification_cost = total_fp_cost + total_fn_cost\n",
    "\n",
    "# Calculate value if we could prevent cancellations\n",
    "potential_saved_revenue = TP * AVERAGE_BOOKING_VALUE\n",
    "retention_cost_for_tp = TP * RETENTION_COST\n",
    "net_value = potential_saved_revenue - retention_cost_for_tp - total_fp_cost\n",
    "\n",
    "print(f\"\\n Financial Impact Analysis:\")\n",
    "print(f\"Total misclassification cost: ${total_misclassification_cost:,.2f}\")\n",
    "print(f\"  â€¢ False Positive cost ({FP} Ã— ${FALSE_POSITIVE_COST}): ${total_fp_cost:,.2f}\")\n",
    "print(f\"  â€¢ False Negative cost ({FN} Ã— ${FALSE_NEGATIVE_COST}): ${total_fn_cost:,.2f}\")\n",
    "print(f\"\\nPotential value with perfect predictions:\")\n",
    "print(f\"  Revenue saved from prevented cancellations: ${potential_saved_revenue:,.2f}\")\n",
    "print(f\"  Cost of retention actions: ${retention_cost_for_tp:,.2f}\")\n",
    "print(f\"  Net value (saved - costs): ${net_value:,.2f}\")\n",
    "\n",
    "# Calculate ROI\n",
    "if total_misclassification_cost > 0:\n",
    "    roi = (net_value / total_misclassification_cost) * 100\n",
    "    print(f\"\\nðŸ“ˆ Return on Investment (ROI): {roi:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc38cd1-b393-407d-bd05-a3049167fe28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance by segments (Fairness/Bias Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f24bdd0-713a-464b-b05f-c1ef9fd31e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAIRNESS / BIAS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure we have the necessary columns\n",
    "if \"hotel\" in predictions.columns:\n",
    "    # Performance by hotel type\n",
    "    print(\"\\nPerformance by Hotel Type:\")\n",
    "    hotel_perf = predictions.groupBy(\"hotel\").agg(\n",
    "        count(\"*\").alias(\"total_bookings\"),\n",
    "        avg(\"churn\").alias(\"actual_cancel_rate\"),\n",
    "        avg(\"predicted_label\").alias(\"predicted_cancel_rate\"),\n",
    "        avg(\"prediction_correct\").alias(\"accuracy\"),\n",
    "        sum(\"churn\").alias(\"actual_cancellations\"),\n",
    "        sum(\"predicted_label\").alias(\"predicted_cancellations\")\n",
    "    ).orderBy(\"total_bookings\", ascending=False)\n",
    "    \n",
    "    hotel_perf.show(truncate=False)\n",
    "\n",
    "# Performance by lead time groups\n",
    "print(\"\\nPerformance by Lead Time Group:\")\n",
    "lead_time_perf = predictions.withColumn(\n",
    "    \"lead_time_group\",\n",
    "    when(col(\"lead_time\") <= 7, \"0-7 days\")\n",
    "    .when(col(\"lead_time\") <= 30, \"8-30 days\")\n",
    "    .when(col(\"lead_time\") <= 90, \"31-90 days\")\n",
    "    .when(col(\"lead_time\") <= 180, \"91-180 days\")\n",
    "    .otherwise(\"180+ days\")\n",
    ").groupBy(\"lead_time_group\").agg(\n",
    "    count(\"*\").alias(\"total_bookings\"),\n",
    "    avg(\"churn\").alias(\"actual_cancel_rate\"),\n",
    "    avg(\"predicted_label\").alias(\"predicted_cancel_rate\"),\n",
    "    avg(\"prediction_correct\").alias(\"accuracy\")\n",
    ").orderBy(\"lead_time_group\")\n",
    "\n",
    "lead_time_perf.show(truncate=False)\n",
    "\n",
    "# Performance by deposit type (if available)\n",
    "if \"deposit_type\" in predictions.columns:\n",
    "    print(\"\\nPerformance by Deposit Type:\")\n",
    "    deposit_perf = predictions.groupBy(\"deposit_type\").agg(\n",
    "        count(\"*\").alias(\"total_bookings\"),\n",
    "        avg(\"churn\").alias(\"actual_cancel_rate\"),\n",
    "        avg(\"predicted_label\").alias(\"predicted_cancel_rate\"),\n",
    "        avg(\"prediction_correct\").alias(\"accuracy\")\n",
    "    ).orderBy(\"total_bookings\", ascending=False)\n",
    "    \n",
    "    deposit_perf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f4bc858-b8a2-4afa-81f0-6cc222be8690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model calibration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38705cdf-17fd-439b-8a98-a639a63b6c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL CALIBRATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create probability bins\n",
    "calibration_df = predictions.withColumn(\n",
    "    \"probability_bin\",\n",
    "    when(col(\"churn_probability\") < 0.1, \"0.0-0.1\")\n",
    "    .when(col(\"churn_probability\") < 0.2, \"0.1-0.2\")\n",
    "    .when(col(\"churn_probability\") < 0.3, \"0.2-0.3\")\n",
    "    .when(col(\"churn_probability\") < 0.4, \"0.3-0.4\")\n",
    "    .when(col(\"churn_probability\") < 0.5, \"0.4-0.5\")\n",
    "    .when(col(\"churn_probability\") < 0.6, \"0.5-0.6\")\n",
    "    .when(col(\"churn_probability\") < 0.7, \"0.6-0.7\")\n",
    "    .when(col(\"churn_probability\") < 0.8, \"0.7-0.8\")\n",
    "    .when(col(\"churn_probability\") < 0.9, \"0.8-0.9\")\n",
    "    .otherwise(\"0.9-1.0\")\n",
    ")\n",
    "\n",
    "# Calculate calibration metrics\n",
    "calibration_stats = calibration_df.groupBy(\"probability_bin\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"churn_probability\").alias(\"avg_predicted_prob\"),\n",
    "    avg(\"churn\").alias(\"actual_cancel_rate\"),\n",
    "    (avg(\"churn\") - avg(\"churn_probability\")).alias(\"calibration_error\")\n",
    ").orderBy(\"probability_bin\")\n",
    "\n",
    "print(\"Model Calibration by Probability Bins:\")\n",
    "calibration_stats.show(truncate=False)\n",
    "\n",
    "# Calculate overall calibration error\n",
    "calibration_error = calibration_stats.select(\n",
    "    (sum(abs(col(\"calibration_error\") * col(\"count\"))) / sum(col(\"count\"))).alias(\"mean_abs_calibration_error\")\n",
    ").first()[0]\n",
    "\n",
    "print(f\"\\n Mean Absolute Calibration Error: {calibration_error:.4f}\")\n",
    "print(\"   â€¢ < 0.01: Excellent calibration\")\n",
    "print(\"   â€¢ 0.01-0.03: Good calibration\")\n",
    "print(\"   â€¢ 0.03-0.05: Moderate calibration\")\n",
    "print(\"   â€¢ > 0.05: Poor calibration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756f160a-3e86-4d5d-8baf-edb3abdeac1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018506ad-3250-4b3b-a9b2-97b215b09588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive evaluation summary\n",
    "evaluation_summary = {\n",
    "    \"evaluation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_source\": model_source,\n",
    "    \"test_set_size\": int(total),\n",
    "    \"test_auc\": float(test_auc) if test_auc is not None else None,\n",
    "    \"test_auprc\": float(test_auprc) if test_auprc is not None else None,\n",
    "    \"accuracy\": float(accuracy) if accuracy is not None else None,\n",
    "    \"precision\": float(precision) if precision is not None else None,\n",
    "    \"recall\": float(recall) if recall is not None else None,\n",
    "    \"f1_score\": float(f1_score) if f1_score is not None else None,\n",
    "    \"true_positives\": int(TP),\n",
    "    \"true_negatives\": int(TN),\n",
    "    \"false_positives\": int(FP),\n",
    "    \"false_negatives\": int(FN),\n",
    "    \"optimal_threshold\": float(optimal_threshold) if optimal_threshold is not None else None,\n",
    "    \"calibration_error\": float(calibration_error) if calibration_error is not None else None,\n",
    "    \"total_misclassification_cost\": float(total_misclassification_cost) if total_misclassification_cost is not None else None,\n",
    "    \"potential_net_value\": float(net_value) if net_value is not None else None\n",
    "}\n",
    "\n",
    "# Explicitly define schema to avoid inference errors\n",
    "schema = StructType([\n",
    "    StructField(\"evaluation_timestamp\", StringType(), True),\n",
    "    StructField(\"model_source\", StringType(), True),\n",
    "    StructField(\"test_set_size\", IntegerType(), True),\n",
    "    StructField(\"test_auc\", DoubleType(), True),\n",
    "    StructField(\"test_auprc\", DoubleType(), True),\n",
    "    StructField(\"accuracy\", DoubleType(), True),\n",
    "    StructField(\"precision\", DoubleType(), True),\n",
    "    StructField(\"recall\", DoubleType(), True),\n",
    "    StructField(\"f1_score\", DoubleType(), True),\n",
    "    StructField(\"true_positives\", IntegerType(), True),\n",
    "    StructField(\"true_negatives\", IntegerType(), True),\n",
    "    StructField(\"false_positives\", IntegerType(), True),\n",
    "    StructField(\"false_negatives\", IntegerType(), True),\n",
    "    StructField(\"optimal_threshold\", DoubleType(), True),\n",
    "    StructField(\"calibration_error\", DoubleType(), True),\n",
    "    StructField(\"total_misclassification_cost\", DoubleType(), True),\n",
    "    StructField(\"potential_net_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Convert to DataFrame with explicit schema\n",
    "summary_df = spark.createDataFrame([evaluation_summary], schema=schema)\n",
    "\n",
    "# Save to Delta table\n",
    "try:\n",
    "    summary_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"hotel_catalog.gold.model_evaluation_summary\")\n",
    "    print(\" Evaluation summary saved to hotel_catalog.gold.model_evaluation_summary\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save to Delta table: {e}\")\n",
    "    # Create temp view\n",
    "    summary_df.createOrReplaceTempView(\"model_evaluation_summary_temp\")\n",
    "    print(\"   Created temp view: model_evaluation_summary_temp\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n Evaluation Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in evaluation_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        if key in [\"test_auc\", \"test_auprc\", \"accuracy\", \"precision\", \"recall\", \"f1_score\", \"calibration_error\"]:\n",
    "            print(f\"{key:30s}: {value:.4f}\" if value is not None else f\"{key:30s}: None\")\n",
    "        elif key in [\"total_misclassification_cost\", \"potential_net_value\"]:\n",
    "            print(f\"{key:30s}: ${value:,.2f}\" if value is not None else f\"{key:30s}: None\")\n",
    "        else:\n",
    "            print(f\"{key:30s}: {value}\" if value is not None else f\"{key:30s}: None\")\n",
    "    else:\n",
    "        print(f\"{key:30s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32eeba8d-e1c0-456a-80ab-26e42abd035f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate deployment recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4519e0f-01bd-4fb9-b4a3-cc542c616660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create deployment checklist\n",
    "checklist_items = []\n",
    "\n",
    "# Model performance checklist\n",
    "if test_auc is not None:\n",
    "    if test_auc >= 0.8:\n",
    "        checklist_items.append(\"AUC-ROC >= 0.8 (Excellent)\")\n",
    "    elif test_auc >= 0.7:\n",
    "        checklist_items.append(\"AUC-ROC >= 0.7 (Good)\")\n",
    "    else:\n",
    "        checklist_items.append(f\"AUC-ROC {test_auc:.3f} (Needs improvement)\")\n",
    "else:\n",
    "    checklist_items.append(\"AUC-ROC not available (metrics could not be computed)\")\n",
    "\n",
    "if calibration_error is not None:\n",
    "    if calibration_error < 0.03:\n",
    "        checklist_items.append(\"Good calibration (< 0.03)\")\n",
    "    else:\n",
    "        checklist_items.append(f\"Calibration error {calibration_error:.3f}\")\n",
    "else:\n",
    "    checklist_items.append(\"Calibration error not available\")\n",
    "\n",
    "if net_value is not None:\n",
    "    if net_value > 0:\n",
    "        checklist_items.append(f\"Positive business value: ${net_value:,.2f}\")\n",
    "    else:\n",
    "        checklist_items.append(f\"Negative business value: ${net_value:,.2f}\")\n",
    "else:\n",
    "    checklist_items.append(\"Business value not available\")\n",
    "\n",
    "# Bias/fairness checklist\n",
    "if \"hotel\" in predictions.columns:\n",
    "    hotel_acc = hotel_perf.select(stddev(\"accuracy\")).collect()[0][0]\n",
    "    if hotel_acc is not None and hotel_acc < 0.05:\n",
    "        checklist_items.append(\"Low performance variance across hotels\")\n",
    "    elif hotel_acc is not None:\n",
    "        checklist_items.append(f\"Performance varies across hotels (std: {hotel_acc:.3f})\")\n",
    "    else:\n",
    "        checklist_items.append(\"Hotel accuracy variance not available\")\n",
    "\n",
    "print(\"\\nDeployment Checklist:\")\n",
    "for item in checklist_items:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n Recommendations:\")\n",
    "print(\"1. Model Performance:\")\n",
    "if test_auc is not None:\n",
    "    print(f\"   â€¢ Current AUC: {test_auc:.3f} - {'Ready for production' if test_auc >= 0.75 else 'Needs improvement'}\")\n",
    "else:\n",
    "    print(\"   â€¢ Current AUC: Not available\")\n",
    "print(f\"   â€¢ Optimal threshold: {optimal_threshold:.2f} (adjust based on business needs)\")\n",
    "\n",
    "print(\"\\n2. Business Impact:\")\n",
    "if net_value is not None:\n",
    "    print(f\"   â€¢ Net value per {total:,} bookings: ${net_value:,.2f}\")\n",
    "else:\n",
    "    print(f\"   â€¢ Net value per {total:,} bookings: Not available\")\n",
    "if total_misclassification_cost is not None and net_value is not None and total_misclassification_cost > 0:\n",
    "    print(f\"   â€¢ ROI: {(net_value/total_misclassification_cost*100):.1f}%\")\n",
    "else:\n",
    "    print(f\"   â€¢ ROI: Not available\")\n",
    "\n",
    "print(\"\\n3. Monitoring:\")\n",
    "print(\"   â€¢ Monitor model performance weekly\")\n",
    "print(\"   â€¢ Track calibration drift monthly\")\n",
    "print(\"   â€¢ Set up alerts for performance degradation\")\n",
    "\n",
    "print(\"\\n4. Actionable Insights:\")\n",
    "print(\"   â€¢ High-risk bookings (probability > 0.7): Consider proactive retention\")\n",
    "print(\"   â€¢ Medium-risk (0.4-0.7): Standard monitoring\")\n",
    "print(\"   â€¢ Low-risk (< 0.4): No action needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f00a94-bbf7-4f81-a4b6-d4cc3a3f6b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create monitoring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc0ce10-7ee9-43fb-8910-61ad7e1c6bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MONITORING SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a dataset for monitoring\n",
    "monitoring_data = predictions.select(\n",
    "    \"hotel\",\n",
    "    \"churn\",\n",
    "    \"predicted_label\",\n",
    "    \"churn_probability\",\n",
    "    \"prediction_correct\",\n",
    "    current_date().alias(\"evaluation_date\"),\n",
    "    lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\")).alias(\"evaluation_batch\")\n",
    ")\n",
    "\n",
    "# Add risk categories\n",
    "monitoring_data = monitoring_data.withColumn(\n",
    "    \"risk_category\",\n",
    "    when(col(\"churn_probability\") < 0.3, \"Low\")\n",
    "    .when(col(\"churn_probability\") < 0.7, \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "# Save monitoring data\n",
    "try:\n",
    "    monitoring_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"evaluation_date\") \\\n",
    "        .saveAsTable(\"hotel_catalog.gold.model_monitoring_predictions\")\n",
    "    print(\"Monitoring data saved with partitioning by date\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save monitoring data: {e}\")\n",
    "    # Create sample for demonstration\n",
    "    monitoring_sample = monitoring_data.limit(1000)\n",
    "    monitoring_sample.createOrReplaceTempView(\"model_monitoring_sample\")\n",
    "    print(\"Created temp view: model_monitoring_sample\")\n",
    "\n",
    "# Create monitoring summary\n",
    "monitoring_summary = monitoring_data.groupBy(\"risk_category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"churn\").alias(\"actual_cancel_rate\"),\n",
    "    avg(\"predicted_label\").alias(\"predicted_cancel_rate\"),\n",
    "    avg(\"prediction_correct\").alias(\"accuracy\")\n",
    ").orderBy(\"risk_category\")\n",
    "\n",
    "print(\"\\nRisk Category Summary:\")\n",
    "monitoring_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b186e65-686f-4a2f-aafd-4549cacd1b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate SQL queries for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e178435-55f0-4975-beba-978b290e793c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DASHBOARD QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SQL queries for monitoring dashboard\n",
    "dashboard_queries = \"\"\"\n",
    "-- 1. Daily Performance Metrics\n",
    "SELECT \n",
    "    evaluation_date,\n",
    "    COUNT(*) as total_predictions,\n",
    "    AVG(is_canceled) as actual_cancel_rate,\n",
    "    AVG(predicted_label) as predicted_cancel_rate,\n",
    "    AVG(prediction_correct) as accuracy,\n",
    "    SUM(CASE WHEN predicted_label = 1 AND is_canceled = 1 THEN 1 ELSE 0 END) as true_positives,\n",
    "    SUM(CASE WHEN predicted_label = 1 AND is_canceled = 0 THEN 1 ELSE 0 END) as false_positives\n",
    "FROM hotel_catalog.gold.model_monitoring_predictions\n",
    "GROUP BY evaluation_date\n",
    "ORDER BY evaluation_date DESC;\n",
    "\n",
    "-- 2. Risk Category Distribution\n",
    "SELECT \n",
    "    risk_category,\n",
    "    COUNT(*) as count,\n",
    "    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage,\n",
    "    AVG(churn_probability) as avg_probability,\n",
    "    AVG(is_canceled) as actual_cancel_rate\n",
    "FROM hotel_catalog.gold.model_monitoring_predictions\n",
    "WHERE evaluation_date = CURRENT_DATE()\n",
    "GROUP BY risk_category\n",
    "ORDER BY risk_category;\n",
    "\n",
    "-- 3. High-Risk Bookings for Today\n",
    "SELECT \n",
    "    hotel,\n",
    "    churn_probability,\n",
    "    DATE_ADD(CURRENT_DATE(), lead_time) as estimated_arrival_date\n",
    "FROM hotel_catalog.gold.model_monitoring_predictions\n",
    "WHERE evaluation_date = CURRENT_DATE()\n",
    "    AND risk_category = 'High'\n",
    "    AND predicted_label = 1\n",
    "ORDER BY churn_probability DESC\n",
    "LIMIT 50;\n",
    "\n",
    "-- 4. Model Performance Trend (Last 30 days)\n",
    "SELECT \n",
    "    evaluation_date,\n",
    "    AVG(prediction_correct) as accuracy,\n",
    "    COUNT(*) as prediction_count\n",
    "FROM hotel_catalog.gold.model_monitoring_predictions\n",
    "WHERE evaluation_date >= DATE_ADD(CURRENT_DATE(), -30)\n",
    "GROUP BY evaluation_date\n",
    "ORDER BY evaluation_date;\n",
    "\"\"\"\n",
    "\n",
    "print(\"SQL Queries for Monitoring Dashboard:\")\n",
    "print(dashboard_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0025ab-44c6-46f1-828c-00c4427a4bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1125e3e0-3f47-4bc7-8f8c-c2070069394f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate final report\n",
    "report = f\"\"\"\n",
    "HOTEL CHURN PREDICTION MODEL - EVALUATION REPORT\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "1. EXECUTIVE SUMMARY\n",
    "   â€¢ Model Performance: {'Production Ready' if test_auc is not None and test_auc >= 0.75 else 'Needs Improvement' if test_auc is not None else 'Not available'}\n",
    "   â€¢ Business Value: {f'${net_value:,.2f} potential net value' if net_value is not None else 'Not available'}\n",
    "   â€¢ ROI: {((net_value/total_misclassification_cost)*100 if total_misclassification_cost is not None and net_value is not None and total_misclassification_cost > 0 else 0):.1f}%\n",
    "\n",
    "2. MODEL PERFORMANCE\n",
    "   â€¢ AUC-ROC: {f'{test_auc:.4f}' if test_auc is not None else 'Not available'} {'(Excellent)' if test_auc is not None and test_auc >= 0.8 else '(Good)' if test_auc is not None and test_auc >= 0.7 else '(Needs Improvement)' if test_auc is not None else ''}\n",
    "   â€¢ Precision: {f'{precision:.4f}' if precision is not None else 'Not available'}\n",
    "   â€¢ Recall: {f'{recall:.4f}' if recall is not None else 'Not available'}\n",
    "   â€¢ F1-Score: {f'{f1_score:.4f}' if f1_score is not None else 'Not available'}\n",
    "   â€¢ Accuracy: {f'{accuracy:.4f}' if accuracy is not None else 'Not available'}\n",
    "\n",
    "3. BUSINESS IMPACT\n",
    "   â€¢ High-risk bookings identified: {TP + FP if TP is not None and FP is not None else 'Not available'}\n",
    "   â€¢ Actual cancellations predicted correctly: {TP if TP is not None else 'Not available'}\n",
    "   â€¢ False Alarms (False Positives): {FP if FP is not None else 'Not available'}\n",
    "   â€¢ Missed Cancellations (False Negatives): {FN if FN is not None else 'Not available'}\n",
    "   â€¢ Net Business Value: {f'${net_value:,.2f}' if net_value is not None else 'Not available'}\n",
    "\n",
    "4. RECOMMENDATIONS\n",
    "   â€¢ Use threshold {f'{optimal_threshold:.2f}' if optimal_threshold is not None else 'Not available'} for production\n",
    "   â€¢ Monitor weekly performance metrics\n",
    "   â€¢ Focus retention efforts on High-risk category (> 0.7 probability)\n",
    "   â€¢ Review False Positives to refine model\n",
    "\n",
    "5. MONITORING METRICS TO TRACK\n",
    "   â€¢ Daily AUC degradation (< 0.02 drop)\n",
    "   â€¢ Calibration error (< 0.03)\n",
    "   â€¢ Business value trend (should be positive)\n",
    "   â€¢ Prediction volume by risk category\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report to file (in Databricks FileStore)\n",
    "try:\n",
    "    report_path = f\"/FileStore/tables/hotel_churn_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"\\n Report saved to: {report_path}\")\n",
    "except:\n",
    "    print(\"\\n Could not save report to file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n Model evaluation completed successfully!\")\n",
    "print(\"  Next steps:\")\n",
    "print(\"   1. Review the evaluation summary\")\n",
    "print(\"   2. Implement monitoring dashboard\")\n",
    "print(\"   3. Deploy model with optimal threshold\")\n",
    "print(\"   4. Set up automated retraining pipeline\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_model_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
