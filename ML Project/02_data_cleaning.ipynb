{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "830db9e4-e4bc-4d25-99a8-e6f736cfcacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook 02: Data Cleaning\n",
    "# Clean and preprocess data, handle missing values, create Silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede6c754-e3e2-448f-a546-dd1c110cda69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f6f873-9cab-4b31-a8a0-519c072d3f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import mlflow\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c2c2edf-f14d-46f1-90f4-6d79b3c45015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166143d1-5d4b-49ea-9622-6c0294437b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark = SparkSession.builder.appName(\"Hotel_Churn_Cleaning\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6993e7cb-70eb-416b-9eda-f937e41436b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Dataset From Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c3a1dd-b656-45fb-8256-5daf74d1a88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.table(\"hotel_catalog.bronze.raw_hotel_bookings\")\n",
    "print(f\"Initial record count: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12b0114-ce76-4ef5-a44a-0e1f01a7d620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data quality report - Focus on NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b91497-a875-40b2-88c1-b4c78e1ac67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"Columns with NULL (actual null, not string) values:\")\n",
    "for col_name in df.columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        percentage = (null_count / df.count()) * 100\n",
    "        print(f\"  {col_name}: {null_count:,} NULL values ({percentage:.2f}%)\")\n",
    "\n",
    "# Check for string \"NULL\" values\n",
    "print(\"\\n=== Checking for string 'NULL' values ===\")\n",
    "string_null_columns = {}\n",
    "for col_name in df.columns:\n",
    "    # Check if column contains string \"NULL\"\n",
    "    string_null_count = df.filter(trim(col(col_name)) == \"NULL\").count()\n",
    "    if string_null_count > 0:\n",
    "        string_null_columns[col_name] = string_null_count\n",
    "        print(f\"  {col_name}: {string_null_count:,} string 'NULL' values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33fb1f3e-0168-49af-bd37-fc4dad60a389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handle the 'children' column properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fe7d50-897c-458b-8264-e4e1beaad6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Cleaning 'children' column ===\")\n",
    "\n",
    "# First, show the current data type and sample values\n",
    "print(\"Current data type of 'children' column:\")\n",
    "df.select(\"children\").printSchema()\n",
    "\n",
    "print(\"\\nSample distinct values in 'children' column:\")\n",
    "df.select(\"children\").distinct().show(20)\n",
    "\n",
    "# Clean the children column: Convert string values to integer\n",
    "df_clean = df.withColumn(\n",
    "    \"children_clean\",\n",
    "    when(\n",
    "        col(\"children\").isNull(), 0  # Actual NULL values\n",
    "    ).when(\n",
    "        trim(col(\"children\")) == \"NULL\", 0  # String \"NULL\" values\n",
    "    ).when(\n",
    "        trim(col(\"children\")) == \"NA\", 0  # String \"NA\" values\n",
    "    ).when(\n",
    "        trim(col(\"children\")) == \"\", 0  # Empty strings\n",
    "    ).otherwise(\n",
    "        col(\"children\").cast(IntegerType())  # Convert valid numbers\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the original children column and rename\n",
    "df_clean = df_clean.drop(\"children\").withColumnRenamed(\"children_clean\", \"children\")\n",
    "\n",
    "print(\"\\n'children' column after cleaning:\")\n",
    "df_clean.select(\"children\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "495a2e25-f397-41d4-8369-dd031d21d6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handle 'agent' and 'company' columns (which have NULL string values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d191d938-159e-4ba2-934a-af442b915dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Cleaning 'agent' and 'company' columns ===\")\n",
    "\n",
    "# For 'agent' column\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"agent_clean\",\n",
    "    when(\n",
    "        col(\"agent\").isNull(), 0  # Actual NULL values\n",
    "    ).when(\n",
    "        trim(col(\"agent\")) == \"NULL\", 0  # String \"NULL\" values\n",
    "    ).otherwise(\n",
    "        col(\"agent\").cast(IntegerType())  # Convert to integer\n",
    "    )\n",
    ")\n",
    "\n",
    "# For 'company' column  \n",
    "df_clean = df_clean.withColumn(\n",
    "    \"company_clean\",\n",
    "    when(\n",
    "        col(\"company\").isNull(), 0  # Actual NULL values\n",
    "    ).when(\n",
    "        trim(col(\"company\")) == \"NULL\", 0  # String \"NULL\" values\n",
    "    ).otherwise(\n",
    "        col(\"company\").cast(IntegerType())  # Convert to integer\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop original columns and rename cleaned ones\n",
    "df_clean = df_clean.drop(\"agent\", \"company\") \\\n",
    "    .withColumnRenamed(\"agent_clean\", \"agent\") \\\n",
    "    .withColumnRenamed(\"company_clean\", \"company\")\n",
    "\n",
    "print(\"\\n'agent' column after cleaning:\")\n",
    "df_clean.select(\"agent\").summary().show()\n",
    "\n",
    "print(\"\\n'company' column after cleaning:\")\n",
    "df_clean.select(\"company\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "254d6a27-6d0f-4f27-86be-e85e90cc8802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Handle 'country' column (has some NULL values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252d42a3-f871-48c9-b8ab-a7549bceb4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Cleaning 'country' column ===\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"country_clean\",\n",
    "    when(\n",
    "        (col(\"country\").isNull()) | \n",
    "        (trim(col(\"country\")) == \"NULL\") |\n",
    "        (trim(col(\"country\")) == \"\"),\n",
    "        \"UNK\"  # Use \"UNK\" for unknown/missing country\n",
    "    ).otherwise(col(\"country\"))\n",
    ")\n",
    "\n",
    "df_clean = df_clean.drop(\"country\").withColumnRenamed(\"country_clean\", \"country\")\n",
    "\n",
    "print(\"Distinct countries after cleaning (top 10):\")\n",
    "df_clean.groupBy(\"country\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e1beea-8d93-478d-ac97-b98561c4a9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handle other columns with potential NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e4264d-cc37-438b-95ce-68816ce72c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Handling other columns with NULL values ===\")\n",
    "\n",
    "# List of columns to check for NULL and handle\n",
    "columns_to_check = [\"meal\", \"market_segment\", \"distribution_channel\", \n",
    "                    \"deposit_type\", \"customer_type\", \"reserved_room_type\", \n",
    "                    \"assigned_room_type\"]\n",
    "\n",
    "for col_name in columns_to_check:\n",
    "    if col_name in df_clean.columns:\n",
    "        # Count NULLs before cleaning\n",
    "        null_count_before = df_clean.filter(col(col_name).isNull()).count()\n",
    "        \n",
    "        # Clean the column\n",
    "        df_clean = df_clean.withColumn(\n",
    "            col_name,\n",
    "            when(\n",
    "                col(col_name).isNull(), \"Unknown\"\n",
    "            ).otherwise(col(col_name))\n",
    "        )\n",
    "        \n",
    "        # Count NULLs after cleaning\n",
    "        null_count_after = df_clean.filter(col(col_name).isNull()).count()\n",
    "        \n",
    "        if null_count_before > 0:\n",
    "            print(f\"  {col_name}: {null_count_before} NULL values â†’ replaced with 'Unknown'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfbabaa1-08c0-4ab1-b412-aef9bc5da06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handle Illogical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e738170a-3004-4603-8b13-b2c1fbc0d264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Removing impossible values ===\")\n",
    "\n",
    "# Adults cannot be 0 or negative\n",
    "initial_count = df_clean.count()\n",
    "df_clean = df_clean.filter(col(\"adults\") > 0)\n",
    "removed = initial_count - df_clean.count()\n",
    "print(f\"  Removed {removed} rows where adults <= 0\")\n",
    "\n",
    "# ADR (Average Daily Rate) should be positive or zero\n",
    "initial_count = df_clean.count()\n",
    "df_clean = df_clean.filter(col(\"adr\") >= 0)\n",
    "removed = initial_count - df_clean.count()\n",
    "print(f\"  Removed {removed} rows where adr < 0\")\n",
    "\n",
    "# Lead time should be non-negative\n",
    "initial_count = df_clean.count()\n",
    "df_clean = df_clean.filter(col(\"lead_time\") >= 0)\n",
    "removed = initial_count - df_clean.count()\n",
    "print(f\"  Removed {removed} rows where lead_time < 0\")\n",
    "\n",
    "# Total nights should be at least 1 (can't have 0 nights stay)\n",
    "initial_count = df_clean.count()\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")) >= 1\n",
    ")\n",
    "removed = initial_count - df_clean.count()\n",
    "print(f\"  Removed {removed} rows where total nights < 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22e8719-950d-4736-99fc-d2e3ca5c75c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660be1eb-da1f-4e48-82a9-ef709b2fdc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Creating derived columns ===\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"total_nights\",\n",
    "    col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")\n",
    ").withColumn(\n",
    "    \"total_guests\",\n",
    "    col(\"adults\") + col(\"children\") + col(\"babies\")\n",
    ").withColumn(\n",
    "    \"arrival_date_month_num\",\n",
    "    expr(\"\"\"\n",
    "        CASE lower(arrival_date_month)\n",
    "            WHEN 'january' THEN 1\n",
    "            WHEN 'february' THEN 2\n",
    "            WHEN 'march' THEN 3\n",
    "            WHEN 'april' THEN 4\n",
    "            WHEN 'may' THEN 5\n",
    "            WHEN 'june' THEN 6\n",
    "            WHEN 'july' THEN 7\n",
    "            WHEN 'august' THEN 8\n",
    "            WHEN 'september' THEN 9\n",
    "            WHEN 'october' THEN 10\n",
    "            WHEN 'november' THEN 11\n",
    "            WHEN 'december' THEN 12\n",
    "            ELSE 0\n",
    "        END\n",
    "    \"\"\")\n",
    ").withColumn(\n",
    "    \"total_spend\",\n",
    "    col(\"adr\") * col(\"total_nights\")\n",
    ").withColumn(\n",
    "    \"is_weekend_stay\",\n",
    "    when(col(\"stays_in_weekend_nights\") > 0, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"room_type_match\",\n",
    "    when(col(\"reserved_room_type\") == col(\"assigned_room_type\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Derived columns created: total_nights, total_guests, arrival_date_month_num, total_spend, is_weekend_stay, room_type_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31c7dc3-5efe-40df-8308-0854e7c79696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deduplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dd68bb-3421-4fe9-8e33-12545194bc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# This removes bookings that are likely the same based on key attributes\n",
    "print(\"\\nMethod: Removing likely duplicates based on key attributes\")\n",
    "window_spec = Window.partitionBy(\n",
    "    \"hotel\", \n",
    "    \"lead_time\", \n",
    "    \"arrival_date_year\", \n",
    "    \"arrival_date_month\", \n",
    "    \"arrival_date_day_of_month\",\n",
    "    \"adults\", \n",
    "    \"children\",\n",
    "    \"country\"\n",
    ").orderBy(\"reservation_status_date\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "before_count = df_clean.count()\n",
    "df_clean = df_clean.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "after_count = df_clean.count()\n",
    "duplicates_removed = before_count - after_count\n",
    "print(f\"  Records before deduplication: {before_count:,}\")\n",
    "print(f\"  Records after deduplication: {after_count:,}\")\n",
    "print(f\"  Duplicates removed: {duplicates_removed:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b665971b-e410-4dd3-99e1-b5da3375aa67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Churn Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9627c5bc-b86e-4d4e-a804-40bb3ee4f389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Defining target variable ===\")\n",
    "df_clean = df_clean.withColumnRenamed(\"is_canceled\", \"churn\")\n",
    "\n",
    "# Check churn distribution\n",
    "churn_stats = df_clean.groupBy(\"churn\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    (count(\"*\") / df_clean.count() * 100).alias(\"percentage\")\n",
    ").orderBy(\"churn\")\n",
    "\n",
    "print(\"Churn distribution:\")\n",
    "churn_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaee0bba-fce3-4eeb-bdc8-c60bcabfee47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write Clean Dataset to Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c759440e-52a0-407f-b8c9-addd58c9436f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "silver_table_name = \"hotel_catalog.silver.cleaned_hotel_bookings\"\n",
    "\n",
    "print(f\"\\n=== Writing cleaned data to Silver layer ===\")\n",
    "print(f\"Table: {silver_table_name}\")\n",
    "\n",
    "df_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(silver_table_name)\n",
    "\n",
    "print(\"Data successfully written to Silver layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6ba2bd4-4a90-4d93-bbd7-a0226de92086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final data quality summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae1977c-e435-4f67-a45d-11ccd43ccfe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nRecord Counts:\")\n",
    "print(f\"  Initial records in Bronze: {df.count():,}\")\n",
    "print(f\"  Final records in Silver: {df_clean.count():,}\")\n",
    "print(f\"  Total records removed: {df.count() - df_clean.count():,}\")\n",
    "\n",
    "print(f\"\\nTarget Variable (Churn):\")\n",
    "churn_count = df_clean.filter(col(\"churn\") == 1).count()\n",
    "non_churn_count = df_clean.filter(col(\"churn\") == 0).count()\n",
    "total_count = df_clean.count()\n",
    "print(f\"  Churn cases: {churn_count:,} ({churn_count/total_count*100:.2f}%)\")\n",
    "print(f\"  Non-churn cases: {non_churn_count:,} ({non_churn_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nHotel Distribution:\")\n",
    "hotel_dist = df_clean.groupBy(\"hotel\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    (count(\"*\") / df_clean.count() * 100).alias(\"percentage\"),\n",
    "    avg(\"churn\").alias(\"churn_rate\")\n",
    ").orderBy(desc(\"count\"))\n",
    "hotel_dist.show(truncate=False)\n",
    "\n",
    "print(f\"\\nTime Period:\")\n",
    "date_stats = df_clean.agg(\n",
    "    min(\"arrival_date_year\").alias(\"min_year\"),\n",
    "    max(\"arrival_date_year\").alias(\"max_year\"),\n",
    "    countDistinct(\"arrival_date_month\").alias(\"unique_months\")\n",
    ").collect()[0]\n",
    "print(f\"  Year range: {date_stats['min_year']} to {date_stats['max_year']}\")\n",
    "print(f\"  Unique months: {date_stats['unique_months']}\")\n",
    "\n",
    "print(f\"\\nFinancial Metrics:\")\n",
    "financial_stats = df_clean.agg(\n",
    "    avg(\"adr\").alias(\"avg_daily_rate\"),\n",
    "    avg(\"total_spend\").alias(\"avg_total_spend\"),\n",
    "    avg(\"lead_time\").alias(\"avg_lead_time_days\"),\n",
    "    avg(\"total_nights\").alias(\"avg_stay_length\")\n",
    ").collect()[0]\n",
    "print(f\"  Average Daily Rate: ${financial_stats['avg_daily_rate']:.2f}\")\n",
    "print(f\"  Average Total Spend: ${financial_stats['avg_total_spend']:.2f}\")\n",
    "print(f\"  Average Lead Time: {financial_stats['avg_lead_time_days']:.1f} days\")\n",
    "print(f\"  Average Stay Length: {financial_stats['avg_stay_length']:.1f} nights\")\n",
    "print(f\"\\nNULL Values Check:\")\n",
    "# Check if any NULL values remain\n",
    "remaining_nulls = []\n",
    "for col_name in df_clean.columns:\n",
    "    null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        remaining_nulls.append((col_name, null_count))\n",
    "\n",
    "if remaining_nulls:\n",
    "    print(\"  Warning: Some columns still have NULL values:\")\n",
    "    for col_name, count in remaining_nulls:\n",
    "        print(f\"    {col_name}: {count} NULLs\")\n",
    "else:\n",
    "    print(\" No NULL values found in cleaned data!\")\n",
    "\n",
    "# Cell 14: Display sample of cleaned data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE OF CLEANED DATA (10 records)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_columns = [\n",
    "    \"hotel\", \"churn\", \"lead_time\", \"arrival_date_year\", \n",
    "    \"arrival_date_month\", \"total_nights\", \"total_guests\",\n",
    "    \"adr\", \"country\", \"deposit_type\", \"customer_type\"\n",
    "]\n",
    "\n",
    "df_clean.select(sample_columns).show(10, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CLEANING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data saved to: {silver_table_name}\")\n",
    "print(f\"Total records: {df_clean.count():,}\")\n",
    "print(f\"Churn rate: {churn_count/total_count*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
