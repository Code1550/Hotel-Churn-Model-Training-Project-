{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc62ac2b-a401-43b5-8290-a9baffe13cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook 04: Model Training (Serverless-Compatible)\n",
    "Train models with MLflow tracking, handle class imbalance, and register best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b622a5d-cf3e-4e7d-b09a-f39b85ed05c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c822db67-9f6b-4ae8-9c2c-e7c1254e2f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, \n",
    "    GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc0877b-2446-4b95-a51e-9e569756dcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize with optimization for Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc9f595-80b7-453e-b3a5-0d601b5b4f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hotel_Churn_Model_Training\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3a550f-145c-43a1-a834-040e38eabafc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bde5260-6bea-4e02-a0cf-fbecf61f76ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "try:\n",
    "    # Try to get username from Python\n",
    "    current_user = getpass.getuser()\n",
    "    experiment_name = f\"/Users/{current_user}/hotel_churn_prediction\"\n",
    "except:\n",
    "    # Fallback to shared experiment\n",
    "    experiment_name = \"/Shared/hotel_churn_prediction\"\n",
    "\n",
    "print(f\"Using experiment: {experiment_name}\")\n",
    "\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        print(f\"Created new experiment: {experiment_name}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"Using existing experiment: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up experiment: {e}\")\n",
    "    # Create with a simple name\n",
    "    experiment_name = \"hotel_churn_prediction\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    except:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is not None:\n",
    "            experiment_id = experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"Failed to create or find experiment: {experiment_name}\")\n",
    "            experiment_id = None\n",
    "\n",
    "if experiment_id is not None:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"MLflow Experiment ID: {experiment_id}\")\n",
    "else:\n",
    "    print(\"MLflow Experiment ID could not be determined. Skipping set_experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d386327-0b18-4f9a-9fb5-5bb3537520b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load features from Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172ebcfc-b59c-4b51-8994-fc3631fbea49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\nLoading features from Gold layer...\")\n",
    "\n",
    "# Try different feature tables\n",
    "feature_tables = [\n",
    "    \"hotel_catalog.gold.hotel_features_final\",\n",
    "    \"hotel_catalog.gold.hotel_features_sql\",\n",
    "    \"hotel_catalog.gold.hotel_bookings_features\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for table in feature_tables:\n",
    "    try:\n",
    "        df = spark.table(table)\n",
    "        print(f\"Loaded features from: {table}\")\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if df is None:\n",
    "    print(\"No feature tables found. Creating simple features...\")\n",
    "    # Fallback: Create simple features from silver\n",
    "    silver_df = spark.table(\"hotel_catalog.silver.cleaned_hotel_bookings\")\n",
    "    \n",
    "    # Create basic features\n",
    "    df = silver_df.select(\n",
    "        \"hotel\",\n",
    "        \"chrun\",\n",
    "        \"lead_time\",\n",
    "        \"arrival_date_year\",\n",
    "        \"arrival_date_month\",\n",
    "        \"stays_in_weekend_nights\",\n",
    "        \"stays_in_week_nights\",\n",
    "        \"adults\",\n",
    "        col(\"children\").cast(\"double\").alias(\"children\"),\n",
    "        \"babies\",\n",
    "        \"previous_cancellations\",\n",
    "        \"adr\",\n",
    "        \"deposit_type\",\n",
    "        \"customer_type\"\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Create derived features\n",
    "    df = df.withColumn(\n",
    "        \"total_nights\",\n",
    "        col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")\n",
    "    ).withColumn(\n",
    "        \"total_guests\",\n",
    "        col(\"adults\") + col(\"children\") + col(\"babies\")\n",
    "    ).withColumn(\n",
    "        \"is_weekend_stay\",\n",
    "        when(col(\"stays_in_weekend_nights\") > 0, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Manual encoding\n",
    "    df = df.withColumn(\n",
    "        \"hotel_code\",\n",
    "        when(col(\"hotel\") == \"Resort Hotel\", 0).otherwise(1)\n",
    "    ).withColumn(\n",
    "        \"deposit_code\",\n",
    "        when(col(\"deposit_type\") == \"No Deposit\", 0)\n",
    "        .when(col(\"deposit_type\") == \"Non Refund\", 1)\n",
    "        .otherwise(2)\n",
    "    )\n",
    "    \n",
    "    # Create feature vector manually\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    \n",
    "    feature_cols = [\n",
    "        \"lead_time\", \"total_nights\", \"total_guests\",\n",
    "        \"previous_cancellations\", \"adr\",\n",
    "        \"is_weekend_stay\", \"hotel_code\", \"deposit_code\"\n",
    "    ]\n",
    "    \n",
    "    for col_name in feature_cols:\n",
    "        df = df.fillna({col_name: 0})\n",
    "    \n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    df = assembler.transform(df)\n",
    "\n",
    "print(f\"Total records loaded: {df.count():,}\")\n",
    "\n",
    "# Check data quality\n",
    "print(\"\\n=== Data Quality Check ===\")\n",
    "cancel_count = df.filter(col(\"churn\") == 1).count()\n",
    "total_count = df.count()\n",
    "cancel_rate = cancel_count / total_count\n",
    "\n",
    "print(f\"Total bookings: {total_count:,}\")\n",
    "print(f\"Canceled bookings: {cancel_count:,}\")\n",
    "print(f\"Cancelation rate: {cancel_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb47dcdf-ca55-4f3c-a6da-5744ece91721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c72735-4ad3-4098-bf22-51af52ed0938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Creating Train/Validation/Test Splits ===\")\n",
    "\n",
    "# Simple random split (we'll use this to avoid complexity)\n",
    "splits = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "train_df, val_df, test_df = splits[0], splits[1], splits[2]\n",
    "\n",
    "print(f\"Training set: {train_df.count():,} records\")\n",
    "print(f\"Validation set: {val_df.count():,} records\")\n",
    "print(f\"Test set: {test_df.count():,} records\")\n",
    "\n",
    "# Check class distribution\n",
    "train_pos_rate = train_df.filter(col(\"churn\") == 1).count() / train_df.count()\n",
    "val_pos_rate = val_df.filter(col(\"churn\") == 1).count() / val_df.count()\n",
    "test_pos_rate = test_df.filter(col(\"churn\") == 1).count() / test_df.count()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train: {train_pos_rate:.2%} positive\")\n",
    "print(f\"  Val: {val_pos_rate:.2%} positive\")\n",
    "print(f\"  Test: {test_pos_rate:.2%} positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f132c8-0902-439c-8570-7b60fb0aa11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f40b34-14aa-4a89-975b-829be73b74db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n=== Setting up Model Evaluation ===\")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3301251-343c-4db4-a43e-2ad951d7e2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab3830d3-450c-4afb-9b44-d3cd92022061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Simple\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    \n",
    "    # Create and train model (simplified for Serverless)\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"churn\",\n",
    "        maxIter=20,\n",
    "        regParam=0.01,\n",
    "        elasticNetParam=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr_model = lr.fit(train_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    val_predictions = lr_model.transform(val_df)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_auc = evaluator_auc.evaluate(val_predictions)\n",
    "    val_pr = evaluator_pr.evaluate(val_predictions)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    val_accuracy = val_predictions.filter(\n",
    "        col(\"prediction\") == col(\"churn\")\n",
    "    ).count() / val_predictions.count()\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_auprc\": val_pr,\n",
    "        \"val_accuracy\": val_accuracy\n",
    "    })\n",
    "    \n",
    "    # Log model with UC volume path for serverless compatibility\n",
    "    mlflow.spark.log_model(lr_model, \"model\", dfs_tmpdir=\"/Volumes/mlflow/tmp\")\n",
    "    \n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    print(f\"Validation AUPRC: {val_pr:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    model_results = {\n",
    "        \"LogisticRegression\": {\n",
    "            \"val_auc\": val_auc,\n",
    "            \"val_auprc\": val_pr,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"run_id\": mlflow.active_run().info.run_id\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf31dc94-9362-4e90-a107-adb3cf257f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98dc07d0-3217-46b7-ad36-1b4439553c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: Random Forest (Lightweight)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_Light\"):\n",
    "    \n",
    "    # Create lightweight model for Serverless\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"churn\",\n",
    "        numTrees=30,  # Reduced for Serverless\n",
    "        maxDepth=5,   # Reduced for Serverless\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"num_trees\": 30,\n",
    "        \"max_depth\": 5,\n",
    "        \"seed\": 42\n",
    "    })\n",
    "    \n",
    "    # Train (no cross-validation to save resources)\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf_model = rf.fit(train_df)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_predictions = rf_model.transform(val_df)\n",
    "    val_auc = evaluator_auc.evaluate(val_predictions)\n",
    "    val_pr = evaluator_pr.evaluate(val_predictions)\n",
    "    val_accuracy = val_predictions.filter(\n",
    "        col(\"prediction\") == col(\"churn\")\n",
    "    ).count() / val_predictions.count()\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_auprc\": val_pr,\n",
    "        \"val_accuracy\": val_accuracy\n",
    "    })\n",
    "    \n",
    "    \n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    print(f\"Validation AUPRC: {val_pr:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    model_results[\"RandomForest\"] = {\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_auprc\": val_pr,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"run_id\": mlflow.active_run().info.run_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc1e54b-b37b-42f4-95ec-affdffbc6574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Model 3 - Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93efe9f0-61b9-41ad-8a43-4d512e493080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: Gradient Boosted Trees\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with mlflow.start_run(run_name=\"GradientBoostedTrees\"):\n",
    "    \n",
    "    # Create model with minimal iterations\n",
    "    gbt = GBTClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"churn\",\n",
    "        maxIter=20,  # Reduced for Serverless\n",
    "        maxDepth=3,  # Reduced for Serverless\n",
    "        stepSize=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"GradientBoostedTrees\",\n",
    "        \"max_iter\": 20,\n",
    "        \"max_depth\": 3,\n",
    "        \"step_size\": 0.1\n",
    "    })\n",
    "    \n",
    "    # Train\n",
    "    print(\"Training Gradient Boosted Trees...\")\n",
    "    gbt_model = gbt.fit(train_df)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_predictions = gbt_model.transform(val_df)\n",
    "    val_auc = evaluator_auc.evaluate(val_predictions)\n",
    "    val_pr = evaluator_pr.evaluate(val_predictions)\n",
    "    val_accuracy = val_predictions.filter(\n",
    "        col(\"prediction\") == col(\"churn\")\n",
    "    ).count() / val_predictions.count()\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_auprc\": val_pr,\n",
    "        \"val_accuracy\": val_accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    print(f\"Validation AUPRC: {val_pr:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    model_results[\"GradientBoostedTrees\"] = {\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_auprc\": val_pr,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"run_id\": mlflow.active_run().info.run_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff45776-6a57-4f81-83d1-e917dbd503cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2144930-890b-42fa-ad1f-a2836d5c31ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model_results' in locals():\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Model':25s} | {'AUC':8s} | {'AUPRC':8s} | {'Accuracy':10s} | Run ID\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_model_name = None\n",
    "    best_run_id = None\n",
    "    \n",
    "    for model_name, results in model_results.items():\n",
    "        print(f\"{model_name:25s} | {results['val_auc']:.4f} | {results['val_auprc']:.4f} | {results['val_accuracy']:.4f} | {results['run_id']}\")\n",
    "        \n",
    "        if results['val_auc'] > best_auc:\n",
    "            best_auc = results['val_auc']\n",
    "            best_model_name = model_name\n",
    "            best_run_id = results['run_id']\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"\\n BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   Validation AUC: {best_auc:.4f}\")\n",
    "    print(f\"   Run ID: {best_run_id}\")\n",
    "else:\n",
    "    print(\"No model results to compare!\")\n",
    "    best_run_id = None\n",
    "    best_model_name = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ced16e-000c-4171-a041-6ff10b1e7d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95917aa1-ab5a-46dc-b356-40d2857004d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL REGISTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if best_run_id:\n",
    "    model_name = \"hotel_churn_predictor\"\n",
    "    \n",
    "    try:\n",
    "        # Register the best model\n",
    "        model_uri = f\"runs:/{best_run_id}/model\"\n",
    "        \n",
    "        registered_model = mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n Model registered successfully!\")\n",
    "        print(f\"   Model Name: {registered_model.name}\")\n",
    "        print(f\"   Model Version: {registered_model.version}\")\n",
    "        \n",
    "        # Add model description\n",
    "        try:\n",
    "            mlflow_client.update_model_version(\n",
    "                name=model_name,\n",
    "                version=registered_model.version,\n",
    "                description=f\"Hotel booking churn predictor. Best model: {best_model_name} with AUC: {best_auc:.4f}\"\n",
    "            )\n",
    "        except:\n",
    "            print(\"   Could not update model description (Serverless limitation)\")\n",
    "        \n",
    "        print(f\"   Model URI: models:/{model_name}/{registered_model.version}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Model registration failed: {e}\")\n",
    "        print(\"Trying alternative registration method...\")\n",
    "        \n",
    "        # Alternative: Save model directly\n",
    "        try:\n",
    "            # Load the best model\n",
    "            if best_model_name == \"LogisticRegression\":\n",
    "                best_model = lr_model\n",
    "            elif best_model_name == \"RandomForest\":\n",
    "                best_model = rf_model\n",
    "            else:\n",
    "                best_model = gbt_model\n",
    "            \n",
    "            # Save model to MLflow without registry\n",
    "            with mlflow.start_run(run_id=best_run_id):\n",
    "                mlflow.spark.save_model(best_model, \"best_model\")\n",
    "                print(f\"Model saved to MLflow run: {best_run_id}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Could not save model: {e2}\")\n",
    "else:\n",
    "    print(\"No best model to register!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "009d57e1-e8ac-4fdf-b49f-40e88389a50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f10e801-556c-4804-a7a2-b2812ae99517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if best_run_id:\n",
    "    try:\n",
    "        # Load the best model from MLflow\n",
    "        model_uri = f\"runs:/{best_run_id}/model\"\n",
    "        best_model = mlflow.spark.load_model(model_uri)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        print(\"Evaluating on test set...\")\n",
    "        test_predictions = best_model.transform(test_df)\n",
    "        \n",
    "        # Calculate test metrics\n",
    "        test_auc = evaluator_auc.evaluate(test_predictions)\n",
    "        test_pr = evaluator_pr.evaluate(test_predictions)\n",
    "        test_accuracy = test_predictions.filter(\n",
    "            col(\"prediction\") == col(\"churn\")\n",
    "        ).count() / test_predictions.count()\n",
    "        \n",
    "        print(f\"\\nTest Set Performance:\")\n",
    "        print(f\"AUC-ROC: {test_auc:.4f}\")\n",
    "        print(f\"AUC-PR: {test_pr:.4f}\")\n",
    "        print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        confusion = test_predictions.groupBy(\"churn\", \"prediction\").count()\n",
    "        confusion_pivot = confusion.groupBy(\"is_canceled\").pivot(\"prediction\").sum(\"count\").fillna(0)\n",
    "        confusion_pivot.show()\n",
    "        \n",
    "        # Log test metrics to MLflow\n",
    "        try:\n",
    "            with mlflow.start_run(run_id=best_run_id):\n",
    "                mlflow.log_metrics({\n",
    "                    \"test_auc\": test_auc,\n",
    "                    \"test_auprc\": test_pr,\n",
    "                    \"test_accuracy\": test_accuracy\n",
    "                })\n",
    "        except:\n",
    "            print(\"Could not log test metrics to MLflow run\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Test evaluation failed: {e}\")\n",
    "        print(\"Using last trained model for evaluation...\")\n",
    "        \n",
    "        # Use the last model if MLflow loading fails\n",
    "        if 'gbt_model' in locals():\n",
    "            test_predictions = gbt_model.transform(test_df)\n",
    "            test_auc = evaluator_auc.evaluate(test_predictions)\n",
    "            print(f\"Test AUC (using GBT): {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad2e6eb-b7a5-4e30-845e-5e850f082faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create model performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b19983e-5722-49d4-b70e-45c015d49292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "if 'model_results' in locals():\n",
    "    for model_name, results in model_results.items():\n",
    "        summary_data.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"val_auc\": float(results[\"val_auc\"]),\n",
    "            \"val_auprc\": float(results[\"val_auprc\"]),\n",
    "            \"val_accuracy\": float(results[\"val_accuracy\"]),\n",
    "            \"run_id\": results[\"run_id\"],\n",
    "            \"training_timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    # Save to Delta table\n",
    "    summary_df = spark.createDataFrame(summary_data)\n",
    "    \n",
    "    try:\n",
    "        summary_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(\"hotel_catalog.gold.model_performance_summary\")\n",
    "        print(\"Model performance summary saved to gold layer\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save summary table: {e}\")\n",
    "        # Create temp view instead\n",
    "        summary_df.createOrReplaceTempView(\"model_performance_summary_temp\")\n",
    "        print(\"Created temporary view: model_performance_summary_temp\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    summary_pd = pd.DataFrame(summary_data)\n",
    "    print(summary_pd[['model_name', 'val_auc', 'val_auprc', 'val_accuracy']].to_string(index=False))\n",
    "\n",
    "# Cell 13: Create simple prediction function\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION FUNCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prediction_code = \"\"\"\n",
    "# Simple function to predict hotel booking cancellations\n",
    "def predict_booking_churn(input_df):\n",
    "    '''\n",
    "    Predict churn for new hotel bookings\n",
    "    \n",
    "    Args:\n",
    "        input_df: Spark DataFrame with the same features as training\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    '''\n",
    "    # Load the model (adjust the URI as needed)\n",
    "    model_uri = \"YOUR_MODEL_URI_HERE\"  # e.g., \"runs:/<run_id>/model\"\n",
    "    model = mlflow.spark.load_model(model_uri)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(input_df)\n",
    "    \n",
    "    # Add probability column\n",
    "    predictions = predictions.withColumn(\n",
    "        \"churn_probability\",\n",
    "        expr(\"probability[1]\")  # Probability of class 1 (canceled)\n",
    "    ).withColumn(\n",
    "        \"prediction_label\",\n",
    "        when(col(\"churn_probability\") > 0.5, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# new_bookings = spark.read.table(\"new_hotel_bookings\")\n",
    "# predictions = predict_booking_churn(new_bookings)\n",
    "# predictions.select(\"hotel\", \"lead_time\", \"churn_probability\", \"prediction_label\").show()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prediction function template:\")\n",
    "print(prediction_code)\n",
    "\n",
    "# Cell 14: Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n Model training completed successfully!\")\n",
    "print(f\" Models trained: {len(model_results) if 'model_results' in locals() else 0}\")\n",
    "print(f\" Best model: {best_model_name if 'best_model_name' in locals() else 'N/A'}\")\n",
    "print(f\" Best validation AUC: {best_auc if 'best_auc' in locals() else 'N/A':.4f}\")\n",
    "print(f\" MLflow Experiment: {experiment_name}\")\n",
    "print(f\" Next step: Run Notebook 05 for detailed evaluation and business insights\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
